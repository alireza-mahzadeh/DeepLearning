# -*- coding: utf-8 -*-
"""project_FakeNewsDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QHg4uyjUBqjw5Sbz6UwOzQY3rRPdR2Zf
"""

import pandas as pd
import numpy as np
import random
import math
import re
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from sklearn.utils import shuffle
import nltk
from nltk.corpus import stopwords
import gensim
import tensorflow
from nltk import word_tokenize
from keras.preprocessing.text import one_hot, Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from keras.models import Sequential
from keras.layers import Dense, Flatten, Embedding, Input, LSTM, Conv1D, MaxPool1D, Bidirectional, Dropout,BatchNormalization, SimpleRNN , Layer
from keras.regularizers import l1, l2
from keras import backend as K
from keras.callbacks import History , EarlyStopping
from sklearn.metrics import classification_report , precision_recall_fscore_support
from sklearn.metrics import accuracy_score
import torch
import torch.nn as nn
print(torch.cuda.is_available())
!pip install transformers
!pip install sentencepiece
import sentencepiece
from transformers import BertTokenizer, RobertaTokenizer, RobertaForSequenceClassification , XLMRobertaTokenizer, XLMRobertaForSequenceClassification , DistilBertTokenizer, DistilBertForSequenceClassification, BertConfig, XLMRobertaConfig , BertForSequenceClassification
from torch.utils.data import Dataset ,DataLoader
from torch.nn.utils.rnn import pad_sequence
PRETRAINED_MODEL_NAME = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)
Roberta_tokenizer=XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')
from IPython.display import display, clear_output
from tqdm.notebook import tqdm
from torch.utils.data import Dataset
from keras.regularizers import l1, l2
from google.colab import drive
drive.mount('/content/drive')
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
import sys
from tabulate import tabulate
from scipy.optimize import minimize



class Datapreprocess:
    #Add dataset
    fake_path = '/content/drive/My Drive/Dataset/DS1/Fake.csv'
    true_path = '/content/drive/My Drive/Dataset/DS1/True.csv'
    true_df = pd.read_csv(true_path)
    fake_df = pd.read_csv(fake_path)
    real_titles = true_df.title
    real_titles_ls = [text for text in real_titles]
    # print(alls)
    real_all_words = ' '.join(real_titles)
    # Word_cloud
    # -
    # word_cloud
    # Add Labels to both df
    true_df['true'] = 1
    fake_df['true'] = 0

    # Concat
    df = pd.concat([true_df, fake_df])
    titles = [text for text in df.title]
    # length
    max_len = 0
    titles_len = []
    for title in titles:
        titles_len.append(len(title.split()))
        max_len = max(len(title.split()), max_len)
    # Purify
    df = df.iloc[:, [0, -1]]

    # Shuffle
    df = shuffle(df).reset_index(drop=True)

    # split_data

    train_val_df = df.sample(frac=0.8)
    test_df = df.drop(train_val_df.index)

    train_df = train_val_df.sample(frac=0.8)
    val_df = train_val_df.drop(train_df.index)

    # Reset Index
    train_df = train_df.reset_index(drop=True)
    val_df = val_df.reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)

    # data frame to csv

    train_df.to_csv('/content/drive/MyDrive/Dataset/DS1/train.tsv', sep='\t', index=False)
    val_df.to_csv('/content/drive/MyDrive/Dataset/DS1/val.tsv', sep='\t', index=False)
    test_df.to_csv('/content/drive/MyDrive/Dataset/DS1/test.tsv', sep='\t', index=False)
    df = pd.concat([train_df, val_df, test_df])

    # Data_cleaning
    nltk.download("stopwords")
    # Obtaining Additional Stopwords From nltk
    stop_words = stopwords.words('english')
    stop_words.extend(['from', 'subject', 're', 'edu', 'use'])



class NLP(Datapreprocess):
    @tensorflow.autograph.experimental.do_not_convert
    def __init__(self):
        super().__init__()

    def processing(self):
        #define Tokenization function
        def tokenization(text):
            result = []
            for token in gensim.utils.simple_preprocess(text):
                if token not in gensim.parsing.preprocessing.STOPWORDS and len(
                        token) > 3 and token not in self.stop_words:
                    result.append(token)

            return result

        self.df['clean'] = self.df['title'].apply(tokenization)
        # Obtaining The Total Words Present In The Datase
        list_of_words = []
        for i in self.df.clean:
            for j in i:
                list_of_words.append(j)
        total_words = len(list(set(list_of_words)))

        # Creating A Tokenizer To Tokenize The Words And Create Sequences Of Tokenized Words
        tokenizer = Tokenizer(num_words=total_words)
        tokenizer.fit_on_texts(self.train_df['title'])

        train_sequences = tokenizer.texts_to_sequences(self.train_df['title'])
        val_sequences = tokenizer.texts_to_sequences(self.val_df['title'])
        test_sequences = tokenizer.texts_to_sequences(self.test_df['title'])
        # Adding Padding
        padded_train = pad_sequences(train_sequences, maxlen=42, padding='post', truncating='post')
        padded_val = pad_sequences(val_sequences, maxlen=42, padding='post', truncating='post')
        padded_test = pad_sequences(test_sequences, maxlen=42, padding='post', truncating='post')

        return padded_train, padded_test, padded_val, total_words


class build_model(Datapreprocess):
    @tensorflow.autograph.experimental.do_not_convert
    def __init__(self, total_words, padded_test, padded_train, padded_val, hyperparameters ,Max_iter):
        self.total_words = total_words
        self.padded_test = padded_test
        self.padded_train = padded_train
        self.padded_val = padded_val
        self.hyperparameters = hyperparameters
        self.accuracy_per_epoch = []
        self.val_accuracy_per_epoch = []
        self.accuracy_model1=[]
        self.Max_iter=Max_iter
        # Create a new history object for this run
        self.history = History()
        # Define a list of optimizers to choose from
        optimizers = ['Nadam', 'Adam', 'Adadelta', 'RMSprop']  # Creating model Using CNN-BiLSTM
        optimizer_mapping = {
            0: torch.optim.Adagrad,
            1: torch.optim.Adam,
            2: torch.optim.SGD,
            3: torch.optim.RMSprop,
         }
        selected_optimizer = optimizers[abs(round(hyperparameters[6]))]
        selected_optimizer_class = optimizer_mapping[1]

        if abs(round(hyperparameters[0]))==1:
            print("-----------------------------------Run AT-LSTM Model-------------------------------------------------------------")
            embedding_vector_features = 60
            model = Sequential()
            #Embedding Layer
            model.add(Embedding(total_words, embedding_vector_features, input_length=42))

            model.add(Dropout(abs(hyperparameters[1])))

            # Add the attention layer (similar to Code 1)
            class Attention(Layer):
                def __init__(self, return_sequences=True):
                    self.return_sequences = return_sequences
                    super(Attention, self).__init__()

                def build(self, input_shape):
                    self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1), initializer="normal")
                    self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1), initializer="zeros")
                    super(Attention, self).build(input_shape)

                def call(self, x):
                    e = K.tanh(K.dot(x, self.W) + self.b)
                    a = K.softmax(e, axis=1)
                    output = x * a
                    if self.return_sequences:
                        return output
                    return K.sum(output, axis=1)
            #LSTM layer
            model.add(LSTM(abs(round(hyperparameters[4])), return_sequences=True))  # Add return_sequences=True for attention

            model.add(Attention(return_sequences=True))

            model.add(LSTM(abs(round(hyperparameters[4]))))  # Add return_sequences=True for attention

            model.add(Dropout(abs(hyperparameters[1])))

            # Add L2 regularization to the Dense layer
            model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0042)))  # Adjust regularization strength

            model.compile(loss='binary_crossentropy', optimizer=selected_optimizer, metrics=['accuracy'])

            self.y_train = np.asarray(super().train_df['true'])

            self.y_val = np.asarray(super().val_df['true'])

            # Training the model

            model.fit(padded_train, self.y_train, batch_size=abs(round(hyperparameters[7])), validation_data=(padded_val, self.y_val),epochs=abs(round(hyperparameters[5])) , callbacks=[self.history])
            # Making prediction
            prediction = (model.predict(padded_test) > 0.5).astype("int32")

            # Getting The Accuracy
            self.y_test = np.asarray(super().test_df['true'])

            self.accuracy = accuracy_score(list(self.y_test), prediction)
            self.accuracy_model1.append(self.accuracy)
            self.accuracy_per_epoch = self.history.history['accuracy']  # Training accuracy per epoch
            self.val_accuracy_per_epoch = self.history.history['val_accuracy']  # Validation accuracy per epoch
            self.loss_per_epoch = self.history.history['loss']
            self.val_loss_per_epoch = self.history.history['val_loss']
            self.model_type='AT-LSTM'
            self.labels_true=self.y_test
            self.predictions_val=prediction

        elif abs(round(hyperparameters[0]))==2:
            print("-----------------------------------Run BERT Model-------------------------------------------------------------")

            class FakeNewsDataset(Dataset):
                def __init__(self, mode, tokenizer):
                    assert mode in ['train', 'val', 'test']
                    self.mode=mode
                    file_path = f'/content/drive/MyDrive/Dataset/DS1/{mode}.tsv'
                    self.df = pd.read_csv(file_path, sep='\t').fillna("")
                    self.len = len(self.df)
                    self.tokenizer = tokenizer  # BERT tokenizer


                # Define a function to return a training/testing data sample
                def __getitem__(self, idx):
                    if self.mode == 'test':
                        statement, label = self.df.iloc[idx, :].values
                        label_tensor = torch.tensor(label)
                    else:
                        statement, label = self.df.iloc[idx, :].values
                        label_tensor = torch.tensor(label)

                    # Build tokens for the first sentence and add the separator token [SEP]
                    word_pieces = ['[CLS]']
                    statement = self.tokenizer.tokenize(statement)
                    word_pieces += statement + ['[SEP]']
                    len_st = len(word_pieces)

                    # Convert the entire token sequence to index sequence
                    ids = self.tokenizer.convert_tokens_to_ids(word_pieces)
                    tokens_tensor = torch.tensor(ids)

                    # Set the token positions of the first sentence including [SEP] to 0
                    segments_tensor = torch.tensor([0] * len_st, dtype=torch.long)

                    return (tokens_tensor, segments_tensor, label_tensor)

                def __len__(self):
                    return self.len

             # Initialize Datasets for Transformation

            trainset = FakeNewsDataset('train', tokenizer=tokenizer)
            valset = FakeNewsDataset('val', tokenizer=tokenizer)
            testset = FakeNewsDataset('test', tokenizer=tokenizer)
            self.train_accuracy_BERT = []
            self.train_loss_BERT = []
            self.val_accuracy_BERT = []
            self.val_loss_BERT = []
            # Sampling and Observing Tensors
            # Choose the first sample
            sample_idx = 0

           # Extract the Main text
            statement, label = trainset.df.iloc[sample_idx].values

            # Retrieve the transformed id tensors using the created Dataset
            tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]

            # Convert tokens_tensor back to text
            tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())
            combined_text = " ".join(tokens)

            # Reforming the Dataset to Fit the Model
            # This function takes a list of samples, each containing 3 tensors:
            # - tokens_tensor
            # - segments_tensor
            # - label_tensor
            # It pads the first two tensors with zeros and generates masks_tensors
            def create_mini_batch(samples):
                tokens_tensors = [s[0] for s in samples]
                segments_tensors = [s[1] for s in samples]

                 # Test Data labels
                if samples[0][2] is not None:
                    label_ids = torch.stack([s[2] for s in samples])
                else:
                    label_ids = None

                # Zero Padding
                tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)
                segments_tensors = pad_sequence(segments_tensors, batch_first=True)

                # Attention masks, set positions in tokens_tensors not equal to zero padding to 1
                # so that BERT only pays attention to these positions
                masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)
                masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)

                return tokens_tensors, segments_tensors, masks_tensors, label_ids

            # Initialize a DataLoader that returns 115 training samples at a time
            # Use collate_fn to merge the list of samples into a mini-batch
            BATCH_SIZE = 115
            trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)
            valloader = DataLoader(valset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)
            testloader = DataLoader(testset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)

            data = next(iter(trainloader))

            tokens_tensors, segments_tensors, masks_tensors, label_ids = data

            # Model Construction
            PRETRAINED_MODEL_NAME = "bert-base-uncased"
            NUM_LABELS = 2
            dropout_prob = 0.58  # You can adjust this value as needed
            # Load the default configuration
            config = BertConfig.from_pretrained(PRETRAINED_MODEL_NAME)
            # Modify the dropout values
            config.hidden_dropout_prob = dropout_prob  # Set to your desired value (e.g., 0.9)
            config.attention_probs_dropout_prob = dropout_prob  # Set to your desired value (e.g., 0.9)
            l1_reg_strength = 0.0002  # Adjust the strength of L1 regularization
            l2_reg_strength = 0.0002 # Adjust the strength of L2 regularization

            model = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME,config=config)




            # Display high-level information about the modules in this model
            print("""
                           name             module
                           -----------------------""")
            for name, module in model.named_children():
                if name == "bert":
                    for n, _ in module.named_children():
                        print(f"{name}:{n}")
                else:
                    print("{:16} {}".format(name, module))
            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
            print("device:", device)
            model = model.to(device)

            model.train()

            # Choose an optimizer (e.g., Adam) and set the learning rate and weight decay
            optimizer = selected_optimizer_class(model.parameters(), lr=1e-5 , weight_decay=0.009)

            NUM_EPOCHS =5
            best_val_loss = float('inf')
            patience = 5  # Number of epochs to wait before stopping if the validation loss doesn't improve
            wait = 0  # Counter for patience

            #start forward and backward phase
            for epoch in range(NUM_EPOCHS):
                train_loss = 0.0
                train_acc = 0.0
                true = []
                predictions = []
                # Initialize variables to store validation loss and accuracy
                validation_loss = 0.0
                val_acc = 0.0

                loop = tqdm(trainloader)
                for batch_idx, data in enumerate(loop):
                    tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]

                    # Zero the parameter gradients/ clear previous gradients
                    optimizer.zero_grad()

                    #perform forward pass
                    outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors,
                                    attention_mask=masks_tensors, labels=labels)

                    #calculate loss after forward pass
                    loss = outputs[0]

                    #run backward pass
                    loss.backward()
                    #update weight by using optimizer
                    optimizer.step()

                    #raw output of model
                    logits = outputs[1]

                    #prediction of output by using softmax activation function
                    _, pred = torch.max(logits.data, 1)

                    #calculate training accuracy and loss
                    train_acc += accuracy_score(pred.cpu().tolist(), labels.cpu().tolist())

                    # Record the current batch loss
                    train_loss += loss.item()

                    # if batch_idx == len(trainloader)-1:
                    #     _, acc = get_predictions(model, trainloader, compute_acc=True)

                    loop.set_description(f"Epoch [{epoch + 1}/{NUM_EPOCHS}]")
                    loop.set_postfix(acc=train_acc, loss=train_loss)
                # Calculate average training accuracy and loss for the epoch
                avg_train_acc = train_acc / len(trainloader)
                avg_train_loss = train_loss / len(trainloader)
                self.train_accuracy_BERT.append(avg_train_acc)
                self.train_loss_BERT.append(avg_train_loss)

                #validation loop
                #preventing from gradient computing
                with torch.no_grad():
                     model.eval()
                     for data in testloader:
                           if next(model.parameters()).is_cuda:
                                data = [t.to(device) for t in data if t is not None]

                           tokens_tensors, segments_tensors, masks_tensors, labels =data

                           test_outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors,
                                      attention_mask=masks_tensors, labels=labels)

                           batch_loss = test_outputs.loss.mean()
                           validation_loss += batch_loss.item()

                           logits = test_outputs.logits
                           pred = torch.argmax(logits, dim=1)
                             # Calculate validation accuracy
                           labels = labels.to(device)  # Ensure labels are on the same device
                           val_acc += torch.sum(pred == labels).item()
                           true.extend(labels.cpu().tolist())
                           predictions.extend(pred.cpu().tolist())
                     # Calculate average validation accuracy and loss for the epoch
                     avg_val_acc = val_acc / len(testloader.dataset)
                     avg_val_loss = validation_loss / len(testloader)
                     # Append to the lists for each epoch
                     self.val_accuracy_BERT.append(avg_val_acc)
                     self.val_loss_BERT.append(avg_val_loss)
                         # Check for early stopping
                     if avg_val_loss < best_val_loss:
                        best_val_loss = avg_val_loss
                        wait = 0  # Reset the patience counter
                         # Save your model checkpoint here if needed
                     else:
                         wait += 1
                     if wait >= patience:
                           print("Early stopping after {} epochs without improvement in validation loss.".format(patience))
                           break  # Stop training
            self.accuracy = accuracy_score(predictions, true)
            self.model_type='BERT'
            self.labels_true=true
            self.predictions_val=predictions
        elif abs(round(hyperparameters[0]))==3:
            print("-----------------------------------Run AC-BiLSTM Model-------------------------------------------------------------")
            embedding_vector_features = 60
            model = Sequential()
            #Embedding Layer
            model.add(Embedding(total_words, embedding_vector_features, input_length=42))

            model.add(Dropout(abs(hyperparameters[1])))

            #Convolution Layer
            model.add(Conv1D(abs(round(hyperparameters[2])), abs(round(hyperparameters[3])), activation='relu' , kernel_regularizer=l2(0.0042)))
            model.add(BatchNormalization())
            model.add(MaxPool1D())

            model.add(Conv1D(abs(round(hyperparameters[2])), abs(round(hyperparameters[3])), activation='relu', kernel_regularizer=l2(0.0042)))
            model.add(BatchNormalization())
            #Max pooling Layer
            model.add(MaxPool1D())
            #Define Attention Layer
            class attention(Layer):

                 def __init__(self, return_sequences=True):
                      self.return_sequences = return_sequences
                      super(attention, self).__init__()

                 def build(self, input_shape):
                       self.W = self.add_weight(name="att_weight", shape=(input_shape[-1], 1), initializer="normal")
                       self.b = self.add_weight(name="att_bias", shape=(input_shape[1], 1),  initializer="zeros")

                       super(attention, self).build(input_shape)

                 def call(self, x):
                      e = K.tanh(K.dot(x, self.W) + self.b)
                      a = K.softmax(e, axis=1)
                      output = x * a

                      if self.return_sequences:
                          return output

                      return K.sum(output, axis=1)

            # Bidirectional LSTM layer
            model.add(Bidirectional(LSTM(abs(round(hyperparameters[4])), return_sequences=True)))  # Add return_sequences=True

            model.add(attention(return_sequences=True))  # Add Custom Attention layer here

            model.add(Bidirectional(LSTM(abs(round(hyperparameters[4])))))

            model.add(Dropout(abs(hyperparameters[1])))

            # Add L2 regularization to the Dense layer
            model.add(Dense(1, activation='sigmoid', kernel_regularizer=l2(0.0042)))  # Adjust regularization strength

            model.compile(loss='binary_crossentropy', optimizer=selected_optimizer, metrics=['accuracy'])

            self.y_train = np.asarray(super().train_df['true'])

            self.y_val = np.asarray(super().val_df['true'])

            # Training the model
            model.fit(padded_train, self.y_train, batch_size=abs(round(hyperparameters[7])), validation_data=(padded_val, self.y_val), epochs=abs(round(hyperparameters[5])), callbacks=[self.history])
            # Making prediction
            prediction = (model.predict(padded_test) > 0.5).astype("int32")

            # Getting The Accuracy
            self.y_test = np.asarray(super().test_df['true'])

            self.accuracy = accuracy_score(list(self.y_test), prediction)

            self.accuracy_model1.append(self.accuracy)
            self.accuracy_per_epoch = self.history.history['accuracy']  # Training accuracy per epoch
            self.val_accuracy_per_epoch = self.history.history['val_accuracy']  # Validation accuracy per epoch
            self.loss_per_epoch = self.history.history['loss']
            self.val_loss_per_epoch = self.history.history['val_loss']
            self.model_type='AC-BiLSTM'
            self.labels_true=self.y_test
            self.predictions_val=prediction



        elif abs(round(hyperparameters[0]))==4:
            print("-----------------------------------Run XLM-RoBERTa Model-------------------------------------------------------------")

            class FakeNewsDataset(Dataset):
                def __init__(self, mode, Roberta_tokenizer):
                    assert mode in ['train', 'val', 'test']
                    self.mode = mode
                    file_path = f'/content/drive/MyDrive/Dataset/DS1/{mode}.tsv'
                    self.df = pd.read_csv(file_path, sep='\t').fillna("")
                    self.len = len(self.df)
                    self.Roberta_tokenizer = Roberta_tokenizer  # BERT tokenizer

                # Define a function to return a training/testing data sample
                def __getitem__(self, idx):
                    if self.mode == 'test':
                        statement, label = self.df.iloc[idx, :].values
                        label_tensor = torch.tensor(label)
                    else:
                        statement, label = self.df.iloc[idx, :].values
                        label_tensor = torch.tensor(label)

                    # Build tokens for the first sentence and add the separator token [SEP]
                    word_pieces = ['[CLS]']
                    statement = self.Roberta_tokenizer.tokenize(statement)
                    word_pieces += statement + ['[SEP]']
                    len_st = len(word_pieces)

                    # Convert the entire token sequence to index sequence
                    ids = self.Roberta_tokenizer.convert_tokens_to_ids(word_pieces)
                    tokens_tensor = torch.tensor(ids)

                    # Set the token positions of the first sentence including [SEP] to 0
                    segments_tensor = torch.tensor([0] * len_st, dtype=torch.long)

                    return (tokens_tensor, segments_tensor, label_tensor)

                def __len__(self):
                    return self.len

                # Initialize Datasets for Transformation

            trainset = FakeNewsDataset('train', Roberta_tokenizer=Roberta_tokenizer)
            valset = FakeNewsDataset('val', Roberta_tokenizer=Roberta_tokenizer)
            testset = FakeNewsDataset('test', Roberta_tokenizer=Roberta_tokenizer)
            self.train_accuracy_BERT = []
            self.train_loss_BERT = []
            self.val_accuracy_BERT = []
            self.val_loss_BERT = []
            # Sampling and Observing Tensors
            # Choose the first sample
            sample_idx = 0

            # Extract the Main text
            statement, label = trainset.df.iloc[sample_idx].values

            # Retrieve the transformed id tensors using the created Dataset
            tokens_tensor, segments_tensor, label_tensor = trainset[sample_idx]

             # Convert tokens_tensor back to text
            tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())
            combined_text = " ".join(tokens)

            # Reforming the Dataset to Fit the Model
            # This function takes a list of samples, each containing 3 tensors:
            # - tokens_tensor
            # - segments_tensor
            # - label_tensor
            # It pads the first two tensors with zeros and generates masks_tensors
            def create_mini_batch(samples):
                tokens_tensors = [s[0] for s in samples]
                segments_tensors = [s[1] for s in samples]

                # Test Data labels
                if samples[0][2] is not None:
                    label_ids = torch.stack([s[2] for s in samples])
                else:
                    label_ids = None

                # Zero Padding
                tokens_tensors = pad_sequence(tokens_tensors, batch_first=True)
                segments_tensors = pad_sequence(segments_tensors, batch_first=True)

                # Attention masks, set positions in tokens_tensors not equal to zero padding to 1
                # so that BERT only pays attention to these positions
                masks_tensors = torch.zeros(tokens_tensors.shape, dtype=torch.long)
                masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)

                return tokens_tensors, segments_tensors, masks_tensors, label_ids

            # Initialize a DataLoader that returns 115 training samples at a time
            # Use collate_fn to merge the list of samples into a mini-batch
            BATCH_SIZE = 115
            trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)
            valloader = DataLoader(valset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)
            testloader = DataLoader(testset, batch_size=BATCH_SIZE, collate_fn=create_mini_batch)

            data = next(iter(trainloader))

            tokens_tensors, segments_tensors, masks_tensors, label_ids = data

            # Model Construction
            PRETRAINED_MODEL_NAME = 'xlm-roberta-base'
            NUM_LABELS = 2
            # Add dropout to the XLM-RoBERTa model
            dropout_prob = 0.58  # You can adjust this value as needed
            # Load the default configuration for XLM-RoBERTa
            config = XLMRobertaConfig.from_pretrained(PRETRAINED_MODEL_NAME)
            # Modify the dropout values
            config.hidden_dropout_prob = dropout_prob  # Set to your desired value (e.g., 0.9)
            config.attention_probs_dropout_prob = dropout_prob  # Set to your desired value (e.g., 0.9)
            model = XLMRobertaForSequenceClassification.from_pretrained(PRETRAINED_MODEL_NAME, config=config)


            # Display high-level information about the modules in this model
            print("""
                                       name             module
                                       -----------------------""")
            for name, module in model.named_children():
                if name == "bert":
                    for n, _ in module.named_children():
                        print(f"{name}:{n}")
                else:
                    print("{:16} {}".format(name, module))
            device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
            print("device:", device)
            model = model.to(device)

            model.train()

            # Choose an optimizer (e.g., Adam) and set the learning rate and weight decay
            optimizer = selected_optimizer_class(model.parameters(), lr=1e-4 , weight_decay=0.009)

            NUM_EPOCHS = 5
            best_val_loss = float('inf')
            patience = 5  # Number of epochs to wait before stopping if the validation loss doesn't improve
            wait = 0  # Counter for patience

            for epoch in range(NUM_EPOCHS):
                train_loss = 0.0
                train_acc = 0.0
                true = []
                predictions = []
                # Initialize variables to store validation loss and accuracy
                validation_loss = 0.0
                val_acc = 0.0

                loop = tqdm(trainloader)
                for batch_idx, data in enumerate(loop):
                    tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]

                     # Zero the parameter gradients
                    optimizer.zero_grad()

                    outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors,
                                    attention_mask=masks_tensors, labels=labels)

                    loss = outputs[0]
                    loss.backward()
                    optimizer.step()

                    logits = outputs[1]
                    _, pred = torch.max(logits.data, 1)
                    train_acc += accuracy_score(pred.cpu().tolist(), labels.cpu().tolist())

                    # Record the current batch loss
                    train_loss += loss.item()

                    # if batch_idx == len(trainloader)-1:
                    #     _, acc = get_predictions(model, trainloader, compute_acc=True)

                    loop.set_description(f"Epoch [{epoch + 1}/{NUM_EPOCHS}]")
                    loop.set_postfix(acc=train_acc, loss=train_loss)
                # Calculate average training accuracy and loss for the epoch
                avg_train_acc = train_acc / len(trainloader)
                avg_train_loss = train_loss / len(trainloader)
                self.train_accuracy_BERT.append(avg_train_acc)
                self.train_loss_BERT.append(avg_train_loss)

                with torch.no_grad():
                    model.eval()
                    for data in testloader:
                        if next(model.parameters()).is_cuda:
                            data = [t.to(device) for t in data if t is not None]

                        tokens_tensors, segments_tensors, masks_tensors, labels = data

                        test_outputs = model(input_ids=tokens_tensors, token_type_ids=segments_tensors,
                                             attention_mask=masks_tensors, labels=labels)

                        batch_loss = test_outputs.loss.mean()
                        validation_loss += batch_loss.item()

                        logits = test_outputs.logits
                        pred = torch.argmax(logits, dim=1)
                        # Calculate validation accuracy
                        labels = labels.to(device)  # Ensure labels are on the same device
                        val_acc += torch.sum(pred == labels).item()
                        true.extend(labels.cpu().tolist())
                        predictions.extend(pred.cpu().tolist())
                    # Calculate average validation accuracy and loss for the epoch
                    avg_val_acc = val_acc / len(testloader.dataset)
                    avg_val_loss = validation_loss / len(testloader)
                    # Append to the lists for each epoch
                    self.val_accuracy_BERT.append(avg_val_acc)
                    self.val_loss_BERT.append(avg_val_loss)
                      # Check for early stopping
                    if avg_val_loss < best_val_loss:
                        best_val_loss = avg_val_loss
                        wait = 0  # Reset the patience counter
                         # Save your model checkpoint here if needed
                    else:
                         wait += 1
                    if wait >= patience:
                           print("Early stopping after {} epochs without improvement in validation loss.".format(patience))
                           break  # Stop training
            self.accuracy = accuracy_score(predictions, true)
            self.model_type='XLM-RoBERTa'
            self.labels_true=true
            self.predictions_val=predictions


class FNDH2D(build_model):
     def __init__(self, Max_iter, num_hawks, lb, ub, dim ,padded_train, padded_test, padded_val, total_words):
        self.Max_iter = Max_iter
        self.num_hawks = num_hawks
        self.lb = lb
        self.ub = ub
        self.dim = dim
        self.padded_train=padded_train
        self.padded_test=padded_test
        self.padded_val= padded_val
        self.total_words=total_words
        self.iteration_histories=[]


     def fitness_function(self, hyperparameters):
        self.ob1 = build_model(total_words, padded_test, padded_train, padded_val, hyperparameters , self.Max_iter)
        self.acc = round(self.ob1.accuracy , 5)
        if abs(round(hyperparameters[0]))==1 or abs(round(hyperparameters[0]))==3 :
            # Get precision, recall, F1-score, and support for each class
            precision, recall, f1_score, support = precision_recall_fscore_support(self.ob1.labels_true, self.ob1.predictions_val)
            # Calculate the macro and weighted averages
            self.macro_precision = precision.mean()
            self.macro_recall = recall.mean()
            self.macro_f1_score = f1_score.mean()
            self.loss=sum(self.ob1.val_loss_per_epoch)/len(self.ob1.val_loss_per_epoch)
            if abs(round(hyperparameters[0]))==1 :
                weighted_accuracy=0.201 * self.acc
                weighted_precision=0.196* self.macro_precision
                weighted_recall=0.197*  self.macro_recall
                weighted_f1=0.197*  self.macro_f1_score
                weighted_loss=0.021* (1/self.loss)
            elif abs(round(hyperparameters[0]))==3:
                 weighted_accuracy=0.228 * self.acc
                 weighted_precision=0.224* self.macro_precision
                 weighted_recall=0.225*  self.macro_recall
                 weighted_f1=0.225*  self.macro_f1_score
                 weighted_loss=0.031* (1/self.loss)
            fitness=(weighted_loss + weighted_accuracy + weighted_precision +  weighted_recall + weighted_f1)*100

            fitness=round(fitness , 4)
            iteration_history = {
                'Fitness':fitness,
                'Accuracy': self.acc,
                'training_accuracy': self.ob1.accuracy_per_epoch,
                'validation_accuracy': self.ob1.val_accuracy_per_epoch,
                'loss': self.ob1.loss_per_epoch,
                'validation_loss': self.ob1.val_loss_per_epoch,
                'Model':self.ob1.model_type,
                'labels_true':self.ob1.labels_true,
                'predictions_val':self.ob1.predictions_val
            }
            self.iteration_histories.append(iteration_history)
        elif abs(round(hyperparameters[0]))==2 or abs(round(hyperparameters[0]))==4:
            # Get precision, recall, F1-score, and support for each class
            precision, recall, f1_score, support = precision_recall_fscore_support(self.ob1.labels_true, self.ob1.predictions_val)
            # Calculate the macro and weighted averages
            self.macro_precision = precision.mean()
            self.macro_recall = recall.mean()
            self.macro_f1_score = f1_score.mean()
            self.loss=sum(self.ob1.train_loss_BERT)/len(self.ob1.train_loss_BERT)
            #calculating Fitness using WSM
            weighted_accuracy=0.218 * self.acc
            weighted_precision=0.223 * self.macro_precision
            weighted_recall=0.222 *  self.macro_recall
            weighted_f1=0.222 *  self.macro_f1_score
            weighted_loss=0.027 *(1/self.loss)
            fitness=(weighted_loss + weighted_accuracy + weighted_precision +  weighted_recall + weighted_f1)*100
            fitness=round(fitness , 4)
            iteration_history = {
                'Fitness':fitness,
                'Accuracy': self.acc,
                'training_accuracy': self.ob1.train_accuracy_BERT,
                'validation_accuracy': self.ob1.val_accuracy_BERT,
                'loss': self.ob1.train_loss_BERT,
                'validation_loss': self.ob1.val_loss_BERT,
                'Model':self.ob1.model_type,
                'labels_true':self.ob1.labels_true,
                'predictions_val':self.ob1.predictions_val
            }
            self.iteration_histories.append(iteration_history)

        return fitness

     def levy_flight(self, dim):
        beta = 1.5
        sigma = (math.gamma(1 + beta) * math.sin(math.pi * beta / 2) / (
                math.gamma((1 + beta) / 2) * beta * 2 ** ((beta - 1) / 2))) ** (1 / beta)
        u = 0.01 * np.random.randn(dim) * sigma
        v = np.random.randn(dim)
        zz = np.power(np.absolute(v), (1 / beta))
        step = np.divide(u, zz)
        return step

     def optimize_hyperparameters(self):

        # Initialize random hyperparameters before the main optimization loop
        hyperparameters = []

        for i in range(self.num_hawks):
            # Initialize random hyperparameters
            model_selection = np.random.uniform(0, 1) * (self.ub[0] - self.lb[0]) + self.lb[0]
            dropout_ratio_val = np.random.uniform(0.01, 0.7) * (self.ub[1] - self.lb[1]) + self.lb[1]
            conv1d_size1 = np.random.uniform(0, 1) * (self.ub[2] - self.lb[2]) + self.lb[2]
            conv1d_size2 = np.random.uniform(0, 1) * (self.ub[3] - self.lb[3]) + self.lb[3]
            num_lstm_layers = np.random.uniform(0, 1) * (self.ub[4] - self.lb[4]) + self.lb[4]
            num_epochs = np.random.uniform(0, 1) * (self.ub[5] - self.lb[5]) + self.lb[5]
            # Generate optimizer probability values (example: [0.3, 0.2, 0.2, 0.3])
            optimizer_prob = np.random.randint(0, 3)
            batch_size = np.random.uniform(0, 1) * (self.ub[7] - self.lb[7]) + self.lb[7]
            learning_rate = np.random.uniform(0, 1) * (self.ub[8] - self.lb[8]) + self.lb[8]

            # Append the hyperparameters as a list
            hyperparameters.append(
                [model_selection , dropout_ratio_val, conv1d_size1, conv1d_size2, num_lstm_layers, num_epochs, optimizer_prob, batch_size, learning_rate])

        # Main optimization loop
        convergence_curve = np.zeros(self.Max_iter)
        t = 0

        # Initialize Rabbit's location and energy
        Rabbit_location = np.zeros(self.dim)
        Rabbit_energy = float(-1e9)
        best_fitness_history = []
        best_evaluation_metrices=[]
        rabbit_location_history=[]
        best_hyperparameters_history = []
        iteration_counter = 0

        # Main optimization loop
        while t < self.Max_iter:

            for i in range(self.num_hawks):
                # Clip the hyperparameter values
                hyperparameters[i][0] = np.clip(hyperparameters[i][0], self.lb[0], self.ub[0])
                hyperparameters[i][1] = np.clip(hyperparameters[i][1], self.lb[1], self.ub[1])
                hyperparameters[i][2] = np.clip(hyperparameters[i][2], self.lb[2], self.ub[2])
                hyperparameters[i][3] = np.clip(hyperparameters[i][3], self.lb[3], self.ub[3])
                hyperparameters[i][4] = np.clip(hyperparameters[i][4], self.lb[4], self.ub[4])
                hyperparameters[i][5] = np.clip(hyperparameters[i][5], self.lb[5], self.ub[5])
                hyperparameters[i][6] = np.clip(hyperparameters[i][6], self.lb[6], self.ub[6])
                hyperparameters[i][7] = np.clip(hyperparameters[i][7], self.lb[7], self.ub[7])
                hyperparameters[i][8] = np.clip(hyperparameters[i][8], self.lb[8], self.ub[8])


                # Fitness of current location
                fitness = self.fitness_function(hyperparameters[i])

                # Update the Rabbit's location and energy
                if fitness > Rabbit_energy:
                    Rabbit_energy = fitness
                    Rabbit_location = hyperparameters[i].copy()

                    best_fitness_history.append(Rabbit_energy)
                    best_hyperparameters_history.append(Rabbit_location.copy())
                    best_accuracy=(round(self.acc , 5))*100
                    best_precision=(self.macro_precision)*100
                    best_recall=(self.macro_recall)*100
                    best_f1=(self.macro_f1_score)*100
                    best_loss=(self.loss)*100


            E1 = 2 * (1 - (t / self.Max_iter))

            # Update the location of Harris's Hawks
            for i in range(self.num_hawks):
                E0 = 2 * random.random() - 1
                Escaping_Energy = E1 * E0

                if abs(Escaping_Energy) >= 1:
                    q = random.random()
                    rand_hawk_index = random.randint(0, self.num_hawks - 1)
                    X_rand = hyperparameters[rand_hawk_index]

                    if q < 0.5:
                        # Perch based on other family
                        multiplier = [random.random() for _ in range(self.dim)]
                        hyperparameters[i] = [x - multiplier[idx] * (x - X_rand[idx]) for idx, x in
                                              enumerate(hyperparameters[i])]
                    elif q >= 0.5:
                        # Perch on a random tall tree (random site inside group's home range)
                        home_range = np.mean(hyperparameters, axis=0)
                        multiplier = [random.random() for _ in range(self.dim)]
                        hyperparameters[i] = [x - multiplier[idx] * (x - home_range[idx]) for idx, x in
                                              enumerate(hyperparameters[i])]
                elif abs(Escaping_Energy) < 1:
                    r = random.random()
                    if r >= 0.5 and abs(Escaping_Energy) < 0.5:  # Hard besiege
                        hyperparameters[i] = [x - Escaping_Energy * np.absolute(
                            Rabbit_location[idx] - x) for idx, x in enumerate(hyperparameters[i])]
                    if r >= 0.5 and abs(Escaping_Energy) >= 0.5:
                        # Soft besiege
                        Jump_strength = 2 * (1 - random.random())
                        hyperparameters[i] = [x - Escaping_Energy * np.absolute(
                            Jump_strength * Rabbit_location[idx] - x) for idx, x in enumerate(hyperparameters[i])]
                    if r < 0.5 and abs(Escaping_Energy) >= 0.5:
                        # Soft besiege
                        Jump_strength = 2 * (1 - random.random())
                        X1 = [Rabbit_location[idx] - Escaping_Energy * np.absolute(
                            Jump_strength * Rabbit_location[idx] - x) for idx, x in enumerate(hyperparameters[i])]
                        for b in range(self.dim):
                            X1[b] = np.clip(X1[b], self.lb[b], self.ub[b])
                        if self.fitness_function(X1) > fitness:
                            hyperparameters[i] = X1.copy()
                        else:
                            # Perform Levy flight
                            step = self.levy_flight(self.dim)
                            X2 = [Rabbit_location[idx] - Escaping_Energy * np.absolute(
                                Jump_strength * Rabbit_location[idx] - x) + step[idx] for idx, x in
                                  enumerate(hyperparameters[i])]
                            for c in range(self.dim):
                                X2[c] = np.clip(X2[c], self.lb[c], self.ub[c])
                            if self.fitness_function(X2) > fitness:
                                hyperparameters[i] = X2.copy()
                    if r < 0.5 and abs(Escaping_Energy) < 0.5:  # Hard besiege
                        Jump_strength = 2 * (1 - random.random())
                        X1 = [Rabbit_location[idx] - Escaping_Energy * np.absolute(
                            Jump_strength * Rabbit_location[idx] - np.mean(hyperparameters, axis=0)[idx]) for idx, x in
                              enumerate(hyperparameters[i])]
                        for d in range(self.dim):
                            X1[d] = np.clip(X1[d], self.lb[d], self.ub[d])

                        if self.fitness_function(X1) > fitness:
                            hyperparameters[i] = X1.copy()
                        else:
                            # Perform Levy flight
                            step = self.levy_flight(self.dim)
                            X2 = [Rabbit_location[idx] - Escaping_Energy * np.absolute(
                                Jump_strength * Rabbit_location[idx] - np.mean(hyperparameters, axis=0)[idx]) + step[
                                      idx] for idx, x in enumerate(hyperparameters[i])]
                            for e in range(self.dim):
                                X2[e] = np.clip(X2[e], self.lb[e], self.ub[e])

                            if self.fitness_function(X2) > fitness:
                                hyperparameters[i] = X2.copy()

            convergence_curve[t] = abs(Rabbit_energy)
            if (t % 1 == 0):
                output_text = "At iteration:" + str(t+1) + " the best fitness is: " + f'{str(abs(Rabbit_energy))}%' + " the Best accuracy is:" + f'{str(best_accuracy)}%' +"\n"
                print(output_text)
                rabbit_location_history.extend([Rabbit_location , Rabbit_energy, best_accuracy, best_precision , best_recall , best_f1 , best_loss])
                rabbit_location_history = [item for sublist in rabbit_location_history for item in (sublist if isinstance(sublist, list) else [sublist])]
                rabbit_location_history = [rabbit_location_history[i:i+15] for i in range(0, len(rabbit_location_history), 15)]

            t = t + 1

        #extract model evaluation parameters
        print(rabbit_location_history)
        index = next((i for i, item in enumerate(self.iteration_histories) if item['Fitness'] == abs(Rabbit_energy)), None)
        subsequence = self.iteration_histories[index]
        #extract best_hyperparametrs
        for location in rabbit_location_history:
            # Iterate through each element in the sublist
            for i in range(len(location)):
                # Check the index and apply the appropriate transformation
                if i in [2, 3, 4, 5 ,7 ,8 , 9 , 10 , 11 , 12 , 13 , 14]:
                   if i in [ 9 , 10 , 11 , 12 , 13 , 14]:
                      location[i] = abs(round(location[i] , 5))
                      location[i]=f'{location[i]}%'
                   elif i in [2 , 3 , 4 , 5 , 7 ]:
                        location[i] = abs(round(location[i]))
                   if (location[0] in ['BERT', 'XLM-RoBERTa']) and (i in [2 , 3, 4 ]):
                       location[i]='NULL'
                elif i==6:
                     if location[0] in ['AT-LSTM', 'AC-BiLSTM']:
                        if isinstance(location[i], (np.int64,np.float64)):
                           if abs(round(location[i]))==0:
                                 location[i]='Nadam'
                           elif abs(round(location[i]))==1:
                                   location[i]='Adam'
                           elif abs(round(location[i]))==2:
                                   location[i]='Adadelta'
                           elif abs(round(location[i]))==3:
                                   location[i]='RMSprop'
                     elif location[0] in ['BERT', 'XLM-RoBERTa']:
                          if isinstance(location[i], (np.int64,np.float64)):
                             if abs(round(location[i]))==0:
                                   location[i]='Adagrad'
                             elif abs(round(location[i]))==1:
                                     location[i]='Adam'
                             elif abs(round(location[i]))==2:
                                     location[i]='SGD'
                             elif abs(round(location[i]))==3:
                                     location[i]='RMSprop'

                elif i==0:

                        if abs(round(location[i]))==1:
                              location[i]='AT-LSTM'
                        elif abs(round(location[i]))==2:
                              location[i]='BERT'
                        elif abs(round(location[i]))==3:
                              location[i]='AC-BiLSTM'
                        else:
                              location[i]='XLM-RoBERTa'

                elif i==1:  # Apply abs() to the second element (index 1)
                     location[i] = abs(round(location[i] , 2))

                elif i==8:
                     location[i]=abs(location[i])


        print('Best Deep Learning Model for FakeNews Detection is=',subsequence['Model'],'\n')
        print('Total Validation accuracy After optimization=' , f'{round(best_accuracy,5)}%','\n')
        #create Best hyperparaneters table
        # Define the number of rows (equal to max_iteration)
        max_iteration = self.Max_iter  # You can set this to your desired value
        # Define custom column names
        custom_column_names = ['Iteration','Deep Learning Model', 'Dropout ratio', 'Number of convolutional layer', 'kernel size', 'Number of LSTM layers', 'Epochs', 'Optimizer', 'Batch size', 'Learning Rate' ,'Fitness','Accuracy', 'Precision' ,'Recall' ,'f1-score' , 'loss']
        # Create a list of dictionaries for each record
        def convert_float64(obj):
            if isinstance(obj, list):
               return [convert_float64(item) for item in obj]
            elif isinstance(obj, np.float64):
                 return float(obj)
            elif isinstance(obj, np.int64):
                 return int(obj)
            else:
                 return obj

        # Create a list of dictionaries for each record
        data = []
        for i in range(max_iteration):
            if i < len(rabbit_location_history):
               params = rabbit_location_history[i]
            else:
                params = [None] * len(custom_column_names)

            modified_params = convert_float64(params)
            if isinstance(modified_params, list):
                data.append({'Iteration': i + 1, **dict(zip(custom_column_names[1:], modified_params))})
            else:
                data.append({'Iteration': i + 1, custom_column_names[1]: modified_params})

        # Display the table using tabulate, including the count in a new column
        table = tabulate(data, headers='keys', tablefmt='pretty', showindex=False)
        print('Best Hyperparameters table in each Iteration', '\n')
        # Print the table
        print(table)
        # Calculate X and Y values for the first plot
        x_values_iteration = list(range(1, self.Max_iter + 1))
        y_values_iteration = convergence_curve

        # Plot the first plot
        plt.plot(x_values_iteration, y_values_iteration)
        plt.xlabel('Iteration')
        plt.ylabel('Fitness')
        plt.title('Fitness vs. Iteration')
        plt.xticks(range(1, self.Max_iter + 1))
        plt.show()

        # Determine the range of epochs you want to plot for the second plot
        num_epochs_to_plot = int(abs(round(Rabbit_location[5])))

        # Create a list of discrete epoch values for the x-axis
        x_values_epoch = list(range(1, num_epochs_to_plot + 1))

        # Extract values from the subsequence list
        training_accuracy_values = subsequence['training_accuracy']
        validation_accuracy_values = subsequence['validation_accuracy']
        # Convert accuracy values to percentages
        training_accuracy_percentage = [acc * 100 for acc in training_accuracy_values]
        validation_accuracy_percentage = [acc * 100 for acc in validation_accuracy_values]


        # Plot both curves on the same plot for the second plot
        plt.plot(x_values_epoch, training_accuracy_percentage, label='Training Accuracy')
        plt.plot(x_values_epoch, validation_accuracy_percentage, label='Validation Accuracy')

        plt.xlabel('Epochs')
        plt.ylabel('Accuracy')
        plt.title('Training and Validation Accuracy')

        # Set x-axis tick values to integers
        plt.xticks(x_values_epoch)

        # Add a legend to distinguish the curves for the second plot
        plt.legend()

        plt.show()


        # Slice the accuracy values for the y-axis
        training_loss_values =subsequence['loss']
        validation_loss_values = subsequence['validation_loss']
        # Convert accuracy values to percentages
        training_loss_percentage = [loss * 100 for loss in training_loss_values]
        validation_loss_percentage = [loss * 100 for loss in validation_loss_values]

        # Plot both curves on the same plot for the second plot
        plt.plot(x_values_epoch, validation_loss_percentage, label='Validation loss')
        plt.plot(x_values_epoch, training_loss_percentage, label='Training loss')

        plt.xlabel('Epochs')
        plt.ylabel('loss')
        plt.title('Training and Validation loss')

        # Set x-axis tick values to integers
        plt.xticks(x_values_epoch)

        # Add a legend to distinguish the curves for the second plot
        plt.legend()

        plt.show()

        #slice the Labels and predictions values related to Models
        labels=subsequence['labels_true']
        predictions=subsequence['predictions_val']
        if subsequence['Model']=='AT-LSTM' or subsequence['Model']=='AC-BiLSTM' :
           # Getting The Confusion Matrix
           cm = confusion_matrix(list(labels), predictions)
           plt.figure(figsize = (6, 6))
           sns.heatmap(cm, annot = True)
           plt.show()
           #Getting evaluation metrices
           def print_classification_report_as_percentages(labels, predictions):
               # Generate the classification report
               report = classification_report(labels, predictions)
               # Define a regular expression pattern to match numerical values
               pattern = r'(\d+\.\d+)'
               # Use regex to find and replace numerical values with percentages
               modified_report = re.sub(pattern, lambda match: f'{float(match.group(1)) * 100:.2f}%', report)

               # Print the modified report
               print(modified_report)
           print_classification_report_as_percentages(labels, predictions)
        elif subsequence['Model']=='BERT' or subsequence['Model']=='XLM-RoBERTa' :
            # Getting The Confusion Matrix
            cm = confusion_matrix(labels, predictions, labels=[1, 0], normalize='pred')
            disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Real', 'Fake'])
            disp.plot()
            plt.show()
            # Getting evaluation metrics
            def print_classification_report_as_percentages(labels, predictions):
                # Generate the classification report
                report = classification_report(labels, predictions)
                # Define a regular expression pattern to match numerical values
                pattern = r'(\d+\.\d+)'
                # Use regex to find and replace numerical values with percentages
                modified_report = re.sub(pattern, lambda match: f'{float(match.group(1)) * 100:.2f}%', report)
                # Print the modified report
                print(modified_report)
            print_classification_report_as_percentages(labels, predictions)






o1 = Datapreprocess()
ob = NLP()
padded_train, padded_test, padded_val, total_words = ob.processing()

o2 = FNDH2D(num_hawks=1,
            lb=[3, 0.90, 34, 3, 26, 3, 0 ,100 ,1e-4],
            ub=[3, 0.95, 80, 6, 100, 5, 3 ,150 ,1e-5],
            Max_iter=15,
            dim=9, padded_train=padded_train ,padded_test=padded_test , padded_val=padded_val ,total_words=total_words )



o2.optimize_hyperparameters()



